{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6557d412-7393-4c2c-98aa-f64713a943b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Agent Response:\n",
      " \n",
      "\n",
      "## Step 1: Calculate the difference in accuracy between each model and the target model.\n",
      "- Model 2 (RandomForest): Accuracy = 0.9993 - 0.9980 = 0.0013\n",
      "- Model 3 (LogisticRegression): Accuracy = 0.9980 - 0.9980 = 0.0000\n",
      "- Model 4 (SVC): Accuracy = 0.9990 - 0.9980 = 0.0010\n",
      "- Model 5 (RandomForest): Accuracy = 0.9987 - 0.9980 = 0.0007\n",
      "- Model 6 (LogisticRegression): Accuracy = 0.9983 - 0.9980 = 0.0003\n",
      "- Model 7 (SVC): Accuracy = 0.9987 - 0.9980 = 0.0007\n",
      "- Model 8 (RandomForest): Accuracy = 0.9987 - 0.9980 = 0.0007\n",
      "- Model 9 (LogisticRegression): Accuracy = 0.9980 - 0.9980 = 0.0000\n",
      "\n",
      "## Step 2: Calculate the difference in precision between each model and the target model.\n",
      "- Model 2 (RandomForest): Precision = 1.0000 - 0.0000 = 1.0000\n",
      "- Model 3 (LogisticRegression): Precision = 0.5000 - 0.0000 = 0.5000\n",
      "- Model 4 (SVC): Precision = 0.6667 - 0.0000 = 0.6667\n",
      "- Model 5 (RandomForest): Precision = 0.6667 - 0.0000 = 0.6667\n",
      "- Model 6 (LogisticRegression): Precision = 0.5000 - 0.0000 = 0.5000\n",
      "- Model 7 (SVC): Precision = 0.6667 - 0.0000 = 0.6667\n",
      "- Model 8 (RandomForest): Precision = 0.7500 - 0.0000 = 0.7500\n",
      "- Model 9 (LogisticRegression): Precision = 0.5000 - 0.0000 = 0.5000\n",
      "\n",
      "## Step 3: Calculate the difference in recall between each model and the target model.\n",
      "- Model 2 (RandomForest): Recall = 0.6667 - 0.0000 = 0.6667\n",
      "- Model 3 (LogisticRegression): Recall = 0.1667 - 0.0000 = 0.1667\n",
      "- Model 4 (SVC): Recall = 0.8000 - 0.0000 = 0.8000\n",
      "- Model 5 (RandomForest): Recall = 0.4000 - 0.0000 = 0.4000\n",
      "- Model 6 (LogisticRegression): Recall = 0.4000 - 0.0000 = 0.4000\n",
      "- Model 7 (SVC): Recall = 0.6667 - 0.0000 = 0.6667\n",
      "- Model 8 (RandomForest): Recall = 0.5000 - 0.0000 = 0.5000\n",
      "- Model 9 (LogisticRegression): Recall = 0.3333 - 0.0000 = 0.3333\n",
      "\n",
      "## Step 4: Rank all models from best to worst based on the differences in accuracy, precision, and recall.\n",
      "Ranking by Accuracy:\n",
      "1. Model 2 (RandomForest) with a difference of 0.0013\n",
      "2. Models 5 (RandomForest), 7 (SVC), and 8 (RandomForest) tied with a difference of 0.0007\n",
      "3. Model 4 (SVC) with a difference of 0.0010\n",
      "\n",
      "Ranking by Precision:\n",
      "1. Model 9 (LogisticRegression) is not the best, but it's actually worse than others in this category.\n",
      "2. Models 5 (RandomForest), 6 (LogisticRegression), and 7 (SVC) tied with a difference of 0.6667\n",
      "3. Model 4 (SVC)\n",
      "\n",
      "Ranking by Recall:\n",
      "1. Model 8 (RandomForest)\n",
      "2. Model 4 (SVC)\n",
      "3. Models 5 (RandomForest), 6 (LogisticRegression), and 9 (LogisticRegression) tied\n",
      "\n",
      "## Step 5: Determine the overall ranking of models from best to worst.\n",
      "Based on all three metrics, we can see that:\n",
      "- In terms of accuracy, Model 2 (RandomForest) is slightly better than Models 7 (SVC) and 8 (RandomForest).\n",
      "- For precision, no model stands\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from gpt4all import GPT4All\n",
    "# Connect to local MLflow server\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment = mlflow.get_experiment_by_name(\"Fraud_Detection_Comparison_v1\")\n",
    "\n",
    "# Get all runs from the experiment\n",
    "df_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id]).head(9)\n",
    "\n",
    "\"\"\"# Preview your data\n",
    "print(df_runs[[\n",
    "    \"run_id\", \"params.model\", \n",
    "    \"metrics.accuracy\", \"metrics.precision\", \"metrics.recall\",\n",
    "]])\"\"\"\n",
    "def create_comparison_prompt(df, target_run_id):\n",
    "    target_row = df[df[\"run_id\"] == target_run_id].iloc[0]\n",
    "    prompt = (\n",
    "        f\"Compare the following ML models to the target model (Run ID: {target_run_id}). \"\n",
    "        \"Use accuracy, precision, and recall. Then rank all models from best to worst and explain your ranking.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[\"run_id\"] == target_run_id:\n",
    "            continue  # ⛔ Skip printing the target model again\n",
    "\n",
    "        model_type = row.get(\"params.model\", \"N/A\")\n",
    "        prompt += f\"Model {idx + 1} ({model_type}):\\n\"\n",
    "        prompt += f\" - Run ID: {row['run_id']}\\n\"\n",
    "        prompt += f\" - Model Type: {row.get('params.model', 'N/A')}\\n\"\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\"]:\n",
    "            col = f\"metrics.{metric}\"\n",
    "            if col in row and pd.notnull(row[col]):\n",
    "                prompt += f\" - {metric.capitalize()}: {row[col]:.4f}\\n\"\n",
    "    prompt += \"\\n\"\n",
    "\n",
    "\n",
    "    prompt += (\n",
    "    f\"\\n--- TARGET MODEL METRICS ---\\n\"\n",
    "    f\"Target Model ({df[df['run_id'] == target_run_id].index[0] + 1}):\\n\"\n",
    "    f\" - Run ID: {target_run_id}\\n\"\n",
    "    f\" - Accuracy: {target_row['metrics.accuracy']:.4f}\\n\"\n",
    "    f\" - Precision: {target_row['metrics.precision']:.4f}\\n\"\n",
    "    f\" - Recall: {target_row['metrics.recall']:.4f}\\n\"\n",
    ")\n",
    "    prompt += (\n",
    "    \"\\nRank all models from best to worst compared to the target model \"\n",
    "    \"using both model name and Run ID for clarity.\"\n",
    ")\n",
    "\n",
    "    return prompt\n",
    "        \n",
    "   \n",
    "# Choose a specific run ID to compare others against (e.g., best Logistic Regression model)\n",
    "target_run_id = df_runs.iloc[0][\"run_id\"]  # or pick based on best f1_score, etc.\n",
    "\n",
    "# Load your local model — path must match your installed model\n",
    "model = GPT4All(\"Llama-3.2-3B-Instruct-Q4_0.gguf\")\n",
    "\n",
    "# Generate response from prompt\n",
    "prompt = create_comparison_prompt(df_runs, target_run_id)\n",
    "response = model.generate(prompt, max_tokens=1024, temp=0.7)\n",
    "\n",
    "with open(\"artifacts/ai_model_comparison.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response)\n",
    "\n",
    "print(\"AI Agent Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9346f-0b52-4e86-acff-ded83daaffb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d5d9b-7ffd-44da-af5b-311d4a447ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
