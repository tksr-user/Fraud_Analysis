{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6557d412-7393-4c2c-98aa-f64713a943b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Agent Response:\n",
      " \n",
      "\n",
      "## Step 1: Calculate the difference in accuracy between each model and the target model.\n",
      "To determine which model is better, we need to calculate how well it performs compared to the target model's performance on accuracy.\n",
      "\n",
      "- RandomForest (Run ID: 8dbf6cc47bc342569c784dd40c5bcd6d): Accuracy = 0.9993 - 0.9980 = 0.0013\n",
      "- LogisticRegression (Run ID: e09f7961dd684b408c292ae6610ae4b1): Accuracy = 0.9980 - 0.9980 = 0.0000\n",
      "- SVC (Run ID: fcb26167d4d741df9dc8e33f788dc7c5): Accuracy = 0.9990 - 0.9980 = 0.0010\n",
      "- RandomForest (Run ID: 182a2e49cd3f48d1a5294a6188ff507b): Accuracy = 0.9987 - 0.9980 = 0.0007\n",
      "- LogisticRegression (Run ID: 1ce46e3102934fbb9e10b04e49e833b7): Accuracy = 0.9983 - 0.9980 = 0.0003\n",
      "- SVC (Run ID: ef0abf5224944d5daa3aef7ab4e6f48d): Accuracy = 0.9987 - 0.9980 = 0.0007\n",
      "- RandomForest (Run ID: b75359b284754ceb9f80e6df58272cd6): Accuracy = 0.9987 - 0.9980 = 0.0007\n",
      "- LogisticRegression (Run ID: 35d931bc1f564dd38b32ee6e3bb30242): Accuracy = 0.9980 - 0.9980 = 0.0000\n",
      "\n",
      "## Step 2: Calculate the difference in precision between each model and the target model.\n",
      "To determine which model is better, we also need to calculate how well it performs compared to the target model's performance on precision.\n",
      "\n",
      "- RandomForest (Run ID: 8dbf6cc47bc342569c784dd40c5bcd6d): Precision = 1.0000 - 0.0000 = 1.0000\n",
      "- LogisticRegression (Run ID: e09f7961dd684b408c292ae6610ae4b1): Precision = 0.5000 - 0.0000 = 0.5000\n",
      "- SVC (Run ID: fcb26167d4d741df9dc8e33f788dc7c5): Precision = 0.6667 - 0.0000 = 0.6667\n",
      "- RandomForest (Run ID: 182a2e49cd3f48d1a5294a6188ff507b): Precision = 0.6667 - 0.0000 = 0.6667\n",
      "- LogisticRegression (Run ID: 1ce46e3102934fbb9e10b04e49e833b7): Precision = 0.5000 - 0.0000 = 0.5000\n",
      "- SVC (Run ID: ef0abf5224944d5daa3aef7ab4e6f48d): Precision = 0.6667 - 0.0000 = 0.6667\n",
      "- RandomForest (Run ID: b75359b284754ceb9f80e6df58272cd6): Precision = 0.7500 - 0.0000 = 0.7500\n",
      "- LogisticRegression (Run ID: 35d931bc1f564dd38b32ee6e3bb30242): Precision = 0.5000 - 0.0000 = 0.5000\n",
      "\n",
      "## Step 3: Calculate the difference in recall between each model and the target model.\n",
      "To determine which model is better, we also need to calculate how well it performs compared to the target model's performance on recall.\n",
      "\n",
      "- RandomForest (Run ID: 8dbf6cc47bc342569c784dd40c5bcd6d): Recall = 0.6667 - 0.0000 = 0.6667\n",
      "- LogisticRegression (Run ID: e09f7961dd684b408c292ae6610ae4b1): Recall = 0.1667 - 0.0000 = 0.1667\n",
      "- SVC (Run ID: fcb26167d4d741df9dc8e33f788dc7c5): Recall = 0.8000 - 0.0000 = 0.8000\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from gpt4all import GPT4All\n",
    "# Connect to local MLflow server\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment = mlflow.get_experiment_by_name(\"Fraud_Detection_Comparison_v1\")\n",
    "\n",
    "# Get all runs from the experiment\n",
    "df_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id]).head(9)\n",
    "\n",
    "\"\"\"# Preview your data\n",
    "print(df_runs[[\n",
    "    \"run_id\", \"params.model\", \n",
    "    \"metrics.accuracy\", \"metrics.precision\", \"metrics.recall\",\n",
    "]])\"\"\"\n",
    "def create_comparison_prompt(df, target_run_id):\n",
    "    target_row = df[df[\"run_id\"] == target_run_id].iloc[0]\n",
    "    prompt = (\n",
    "        f\"Compare the following ML models to the target model (Run ID: {target_run_id}). \"\n",
    "        \"Use accuracy, precision, and recall. Then rank all models from best to worst and explain your ranking.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[\"run_id\"] == target_run_id:\n",
    "            continue  # ⛔ Skip printing the target model again\n",
    "\n",
    "        model_type = row.get(\"params.model\", \"N/A\")\n",
    "        prompt += f\"Model {idx + 1} ({model_type}):\\n\"\n",
    "        prompt += f\" - Run ID: {row['run_id']}\\n\"\n",
    "        prompt += f\" - Model Type: {row.get('params.model', 'N/A')}\\n\"\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\"]:\n",
    "            col = f\"metrics.{metric}\"\n",
    "            if col in row and pd.notnull(row[col]):\n",
    "                prompt += f\" - {metric.capitalize()}: {row[col]:.4f}\\n\"\n",
    "    prompt += \"\\n\"\n",
    "\n",
    "\n",
    "    prompt += (\n",
    "    f\"\\n--- TARGET MODEL METRICS ---\\n\"\n",
    "    f\"Target Model ({df[df['run_id'] == target_run_id].index[0] + 1}):\\n\"\n",
    "    f\" - Run ID: {target_run_id}\\n\"\n",
    "    f\" - Accuracy: {target_row['metrics.accuracy']:.4f}\\n\"\n",
    "    f\" - Precision: {target_row['metrics.precision']:.4f}\\n\"\n",
    "    f\" - Recall: {target_row['metrics.recall']:.4f}\\n\"\n",
    ")\n",
    "    prompt += (\n",
    "    \"\\nRank all models from best to worst compared to the target model \"\n",
    "    \"using both model name and Run ID for clarity.\"\n",
    ")\n",
    "\n",
    "    return prompt\n",
    "        \n",
    "   \n",
    "# Choose a specific run ID to compare others against (e.g., best Logistic Regression model)\n",
    "target_run_id = df_runs.iloc[0][\"run_id\"]  # or pick based on best f1_score, etc.\n",
    "\n",
    "# Load your local model — path must match your installed model\n",
    "model = GPT4All(\"Llama-3.2-3B-Instruct-Q4_0.gguf\")\n",
    "\n",
    "# Generate response from prompt\n",
    "prompt = create_comparison_prompt(df_runs, target_run_id)\n",
    "response = model.generate(prompt, max_tokens=1024, temp=0.7)\n",
    "\n",
    "with open(\"artifacts/ai_model_comparison.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response)\n",
    "\n",
    "print(\"AI Agent Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9346f-0b52-4e86-acff-ded83daaffb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
