{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f644cb0-21d8-492c-832f-b047dbbad38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š GPT-4LL Agent's Ranking:\n",
      " Based on the provided metrics, here's a comparison of the five models:\n",
      "\n",
      "**Model Comparison**\n",
      "\n",
      "| **Metric** | **Model 11 (RandomForest)** | **Model 32 (RandomForest)** | **Model 102 (RandomForest)** | **Model 120 (RandomForest)** | **Model 16 (SVC)** |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "| Accuracy | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 0.9997 |\n",
      "| Precision | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 |\n",
      "| Recall | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 0.7500 |\n",
      "\n",
      "**Key Observations**\n",
      "\n",
      "* All four RandomForest models have an accuracy of 1.0000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from gpt4all import GPT4All\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment_name = \"Fraud_Detection_Comparison\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "df_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "# Sort top 5 runs by f1_score\n",
    "df_sorted = df_runs.sort_values(by=\"metrics.accuracy\", ascending=False)\n",
    "def create_model_prompt(df):\n",
    "    prompt = \"Compare the following ML models based on their available metrics:\\n\\n\"\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt += f\"Model {idx + 1}:\\n\"\n",
    "        prompt += f\" - Run ID: {row['run_id']}\\n\"\n",
    "        prompt += f\" - Model: {row.get('params.model', 'N/A')}\\n\"\n",
    "\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
    "            col = f\"metrics.{metric}\"\n",
    "            if col in row and pd.notnull(row[col]):\n",
    "                prompt += f\" - {metric.capitalize()}: {row[col]:.4f}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "model = GPT4All(\"Llama-3.2-3B-Instruct-Q4_0.gguf\")  # adjust to your model name\n",
    "\n",
    "prompt = create_model_prompt(df_sorted.head(5))  # Compare top 5 models\n",
    "\n",
    "with model.chat_session():\n",
    "   response = model.generate(prompt)\n",
    "print(\"ðŸ“Š GPT-4LL Agent's Ranking:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e36fe1-5e29-4903-b5b2-4bba40b450a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metrics.precision', 'metrics.recall', 'metrics.accuracy', 'metrics.num_rows']\n"
     ]
    }
   ],
   "source": [
    "print(df_runs.columns[df_runs.columns.str.startswith(\"metrics.\")].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "879c6000-738f-4b43-bcbd-263045e60e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpt4all in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gpt4all) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gpt4all) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! Pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87398784-c024-41ef-b903-cd245ac94965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
