{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f644cb0-21d8-492c-832f-b047dbbad38c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT4All' object has no attribute 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m prompt = create_model_prompt(df_sorted.head(\u001b[32m5\u001b[39m))  \u001b[38;5;66;03m# Compare top 5 models\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m model.chat_session():\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprompt\u001b[49m(prompt)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š GPT-4LL Agent\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms Ranking:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, response)\n",
      "\u001b[31mAttributeError\u001b[39m: 'GPT4All' object has no attribute 'prompt'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from gpt4all import GPT4All\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment_name = \"Fraud_Detection_Comparison\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "df_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "# Sort top 5 runs by f1_score\n",
    "df_sorted = df_runs.sort_values(by=\"metrics.accuracy\", ascending=False)\n",
    "def create_model_prompt(df):\n",
    "    prompt = \"Compare the following ML models based on their available metrics:\\n\\n\"\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt += f\"Model {idx + 1}:\\n\"\n",
    "        prompt += f\" - Run ID: {row['run_id']}\\n\"\n",
    "        prompt += f\" - Model: {row.get('params.model', 'N/A')}\\n\"\n",
    "\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
    "            col = f\"metrics.{metric}\"\n",
    "            if col in row and pd.notnull(row[col]):\n",
    "                prompt += f\" - {metric.capitalize()}: {row[col]:.4f}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "model = GPT4All(\"Llama-3.2-3B-Instruct-Q4_0.gguf\")  # adjust to your model name\n",
    "\n",
    "prompt = create_model_prompt(df_sorted.head(5))  # Compare top 5 models\n",
    "\n",
    "with model.chat_session():\n",
    "   response = model.generate(prompt)\n",
    "    print(\"ðŸ“Š GPT-4LL Agent's Ranking:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e36fe1-5e29-4903-b5b2-4bba40b450a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metrics.precision', 'metrics.recall', 'metrics.accuracy', 'metrics.num_rows']\n"
     ]
    }
   ],
   "source": [
    "print(df_runs.columns[df_runs.columns.str.startswith(\"metrics.\")].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "879c6000-738f-4b43-bcbd-263045e60e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpt4all in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gpt4all) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gpt4all) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gpt4all) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\aman1\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! Pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87398784-c024-41ef-b903-cd245ac94965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
